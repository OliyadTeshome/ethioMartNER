{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from telethon import TelegramClient, events\n",
    "from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "# Add src to Python path to enable imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to Python path\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir.parent / 'src'\n",
    "sys.path.insert(0, str(src_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Create necessary directories\n",
    "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('../data/raw/photos').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Telegram API configuration - load directly from environment\n",
    "API_ID = os.getenv('TELEGRAM_API_ID')\n",
    "API_HASH = os.getenv('TELEGRAM_API_HASH')\n",
    "PHONE = os.getenv('TELEGRAM_PHONE_NUMBER')\n",
    "\n",
    "# File paths\n",
    "CHANNELS_FILE = '../data/raw/channels_to_crawl.xlsx'\n",
    "OUTPUT_RAW_FILE = '../data/raw/telegram_data.csv'\n",
    "OUTPUT_PROCESSED_FILE = '../data/processed/processed_telegram_data.csv'\n",
    "PHOTOS_DIR = '../data/raw/photos'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Telegram channel names from Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_channel_list(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read Telegram channel names from Excel file\n",
    "    \"\"\"\n",
    "    # Use only the specified channels\n",
    "    channels = [\n",
    "        '@ZemenExpress',\n",
    "        '@ethio_brand_collection', \n",
    "        '@Leyueqa',\n",
    "        '@modernshoppingcenter',\n",
    "        '@qnashcom',\n",
    "        '@MerttEka'\n",
    "    ]\n",
    "    print(f\"Found {len(channels)} channels: {channels}\")\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape messages from Telegram channels using Telethon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_telegram_channels(channels: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape messages from Telegram channels using Telethon\n",
    "    \"\"\"\n",
    "    client = TelegramClient('scraping_session', API_ID, API_HASH)\n",
    "    \n",
    "    all_messages = []\n",
    "    \n",
    "    try:\n",
    "        await client.start()\n",
    "        print(\"Connected to Telegram\")\n",
    "        \n",
    "        for channel in channels:\n",
    "            try:\n",
    "                print(f\"Scraping channel: {channel}\")\n",
    "                entity = await client.get_entity(channel)\n",
    "                channel_title = entity.title\n",
    "                \n",
    "                async for message in client.iter_messages(entity, limit=1000):\n",
    "                    if message.message:  # Only process messages with text\n",
    "                        media_path = None\n",
    "                        \n",
    "                        # Handle media downloads\n",
    "                        if message.media:\n",
    "                            if hasattr(message.media, 'photo'):\n",
    "                                filename = f\"{channel.replace('@', '')}_{message.id}.jpg\"\n",
    "                                media_path = os.path.join(PHOTOS_DIR, filename)\n",
    "                                await client.download_media(message.media, media_path)\n",
    "                        \n",
    "                        message_data = {\n",
    "                            'channel': channel_title,\n",
    "                            'channel_username': channel,\n",
    "                            'sender_id': message.sender_id,\n",
    "                            'timestamp': message.date,\n",
    "                            'message_text': message.message,\n",
    "                            'message_id': message.id,\n",
    "                            'media_path': media_path\n",
    "                        }\n",
    "                        all_messages.append(message_data)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping channel {channel}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    finally:\n",
    "        await client.disconnect()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_messages)\n",
    "    print(f\"Scraped {len(df)} messages from {len(channels)} channels\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and normalize Amharic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amharic_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize Amharic text\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove emojis and special characters (keeping Amharic characters)\n",
    "    # Amharic Unicode range: \\u1200-\\u137F\n",
    "    text = re.sub(r'[^\\u1200-\\u137F\\u0020\\u0027\\u0022\\u002E\\u002C\\u003F\\u0021\\u003A\\u003B\\u0028\\u0029\\u002D\\u002F\\u005C\\u0040\\u0023\\u0024\\u0025\\u005E\\u0026\\u002A\\u002B\\u003D\\u005B\\u005D\\u007B\\u007D\\u007C\\u003C\\u003E\\u007E\\u0060\\u0027\\u0022]', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_amharic_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Basic tokenization for Amharic text using whitespace splitting\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Split by whitespace and filter out empty tokens\n",
    "    tokens = [token.strip() for token in text.split() if token.strip()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the scraped Telegram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_telegram_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the scraped Telegram data\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Clean the message text\n",
    "    processed_df['cleaned_text'] = processed_df['message_text'].apply(clean_amharic_text)\n",
    "    \n",
    "    # Tokenize the cleaned text\n",
    "    processed_df['tokens'] = processed_df['cleaned_text'].apply(tokenize_amharic_text)\n",
    "    \n",
    "    # Add token count\n",
    "    processed_df['token_count'] = processed_df['tokens'].apply(len)\n",
    "    \n",
    "    # Filter out messages with no meaningful content after cleaning\n",
    "    processed_df = processed_df[processed_df['cleaned_text'].str.len() > 0]\n",
    "    \n",
    "    # Select final columns for output\n",
    "    final_columns = ['channel', 'sender_id', 'timestamp', 'message_text', 'cleaned_text', 'token_count']\n",
    "    processed_df = processed_df[final_columns]\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main pipeline for data ingestion and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_ingestion_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline for data ingestion and preprocessing\n",
    "    \"\"\"\n",
    "    print(\"Starting Telegram data ingestion pipeline...\")\n",
    "    \n",
    "    # Step 1: Read channel list\n",
    "    channels = read_channel_list(CHANNELS_FILE)\n",
    "    if not channels:\n",
    "        print(\"No channels found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Scrape Telegram data\n",
    "    print(\"Scraping Telegram channels...\")\n",
    "    raw_df = await scrape_telegram_channels(channels)\n",
    "    \n",
    "    if raw_df.empty:\n",
    "        print(\"No data scraped. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Save raw data\n",
    "    raw_df.to_csv(OUTPUT_RAW_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"Raw data saved to {OUTPUT_RAW_FILE}\")\n",
    "    \n",
    "    # Step 3: Preprocess data\n",
    "    print(\"Preprocessing data...\")\n",
    "    processed_df = preprocess_telegram_data(raw_df)\n",
    "    \n",
    "    # Save processed data\n",
    "    processed_df.to_csv(OUTPUT_PROCESSED_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"Processed data saved to {OUTPUT_PROCESSED_FILE}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Pipeline Summary ===\")\n",
    "    print(f\"Channels processed: {len(channels)}\")\n",
    "    print(f\"Raw messages scraped: {len(raw_df)}\")\n",
    "    print(f\"Processed messages: {len(processed_df)}\")\n",
    "    print(f\"Average tokens per message: {processed_df['token_count'].mean():.2f}\")\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Telegram data ingestion pipeline...\n",
      "Found 6 channels: ['@ZemenExpress', '@ethio_brand_collection', '@Leyueqa', '@modernshoppingcenter', '@qnashcom', '@MerttEka']\n",
      "Scraping Telegram channels...\n",
      "Signed in successfully as Olyad; remember to not break the ToS or you will risk an account ban!\n",
      "Connected to Telegram\n",
      "Scraping channel: @ZemenExpress\n",
      "Scraping channel: @ethio_brand_collection\n",
      "Scraping channel: @Leyueqa\n",
      "Scraping channel: @modernshoppingcenter\n",
      "Scraping channel: @qnashcom\n",
      "Scraping channel: @MerttEka\n",
      "Scraped 3689 messages from 6 channels\n",
      "Raw data saved to ../data/raw/telegram_data.csv\n",
      "Preprocessing data...\n",
      "Processed data saved to ../data/processed/processed_telegram_data.csv\n",
      "\n",
      "=== Pipeline Summary ===\n",
      "Channels processed: 6\n",
      "Raw messages scraped: 3689\n",
      "Processed messages: 3683\n",
      "Average tokens per message: 43.66\n",
      "\n",
      "First few processed messages:\n",
      "          channel      sender_id                 timestamp  \\\n",
      "0  Zemen Express® -1001307493052 2025-06-21 16:35:51+00:00   \n",
      "1  Zemen Express® -1001307493052 2025-06-21 08:07:31+00:00   \n",
      "2  Zemen Express® -1001307493052 2025-06-21 08:07:11+00:00   \n",
      "3  Zemen Express® -1001307493052 2025-06-21 05:42:46+00:00   \n",
      "4  Zemen Express® -1001307493052 2025-06-21 05:42:19+00:00   \n",
      "\n",
      "                                        message_text  \\\n",
      "0  💥💥...................................💥💥\\n\\n📌Sa...   \n",
      "1  💥💥...................................💥💥\\n\\n3pc...   \n",
      "2  💥💥...................................💥💥\\n\\n3pc...   \n",
      "3  💥💥...................................💥💥\\n\\n📌1 ...   \n",
      "4  💥💥...................................💥💥\\n\\n📌1 ...   \n",
      "\n",
      "                                        cleaned_text  token_count  \n",
      "0  ................................... ዋጋ፦ ብር ውስን...           18  \n",
      "1  ................................... በማንኛውም ጠርሙ...           30  \n",
      "2  ................................... በማንኛውም ጠርሙ...           30  \n",
      "3  ................................... ዋጋ፦ ብርውስን ...           17  \n",
      "4  ................................... ዋጋ፦ ብርውስን ...           17  \n"
     ]
    }
   ],
   "source": [
    "# In Jupyter notebooks, we can use await directly instead of asyncio.run()\n",
    "try:\n",
    "    result_df = await main_ingestion_pipeline()\n",
    "    \n",
    "    if result_df is not None and not result_df.empty:\n",
    "        print(\"\\nFirst few processed messages:\")\n",
    "        print(result_df.head())\n",
    "    else:\n",
    "        print(\"Pipeline completed but no data was returned.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error running pipeline: {e}\")\n",
    "    print(\"Please check your API credentials and try again.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
